{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ë¡œë” ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\playdata\\miniconda3\\envs\\imgdataprocess\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\miniconda3\\envs\\imgdataprocess\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\miniconda3\\envs\\imgdataprocess\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\playdata\\miniconda3\\envs\\imgdataprocess\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\miniconda3\\envs\\imgdataprocess\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp310-cp310-win_amd64.whl (204.2 MB)\n",
      "   ---------------------------------------- 0.0/204.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.9/204.2 MB 33.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 11.8/204.2 MB 32.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 19.4/204.2 MB 33.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 27.0/204.2 MB 34.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 34.1/204.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 39.3/204.2 MB 32.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 44.3/204.2 MB 31.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.8/204.2 MB 29.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 53.5/204.2 MB 28.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 55.8/204.2 MB 27.0 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 60.0/204.2 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 64.5/204.2 MB 26.0 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 70.8/204.2 MB 26.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 77.6/204.2 MB 26.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 86.8/204.2 MB 27.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 95.2/204.2 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 101.4/204.2 MB 29.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 106.4/204.2 MB 28.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 113.5/204.2 MB 28.5 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 120.6/204.2 MB 28.6 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 127.1/204.2 MB 28.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 135.0/204.2 MB 29.1 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 143.1/204.2 MB 29.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 149.2/204.2 MB 29.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 156.5/204.2 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 163.3/204.2 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 169.9/204.2 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 176.2/204.2 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.2/204.2 MB 29.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.9/204.2 MB 30.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.1/204.2 MB 30.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.6/204.2 MB 29.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.2 MB 29.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.2/204.2 MB 28.7 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading torchvision-0.21.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.6.0-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 19.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_train = pd.read_csv(\"./data/finishDF.csv\")  \n",
    "df_test = pd.read_csv(\"./data/test_dataset.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œë” ì •ê·œí™”\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ì‚¬ìš©ëª¨ë¸ì— ë§ëŠ” í¬ê¸°ë¡œ..\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]) # ì •ê·œí™”í™”\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë”í™” í´ë˜ìŠ¤\n",
    "\n",
    "class FaceShapeDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        label = int(self.dataframe.iloc[idx]['label']) \n",
    "\n",
    "        # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (OpenCV â†’ PIL ë³€í™˜)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGRì´ë¯€ë¡œ RGB ë³€í™˜\n",
    "        image = Image.fromarray(image)  # NumPy ë°°ì—´ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜(transformsì‚¬ìš©ì— í•„ìš”ìš”)\n",
    "\n",
    "        # ì „ì²˜ë¦¬ ì ìš©\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ ë§¤í•‘: {'Heart': 0, 'Oblong': 1, 'Oval': 2, 'Round': 3, 'Square': 4}\n"
     ]
    }
   ],
   "source": [
    "# ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (Label Encoding)\n",
    "class_map = {label: idx for idx, label in enumerate(df_train['label'].unique())}\n",
    "print(\"í´ë˜ìŠ¤ ë§¤í•‘:\", class_map)  \n",
    "\n",
    "df_train['label'] = df_train['label'].map(class_map)\n",
    "df_test['label'] = df_test['label'].map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œë” ìƒì„±\n",
    "\n",
    "trainset = FaceShapeDataset(df_train, transform=transform)\n",
    "testset = FaceShapeDataset(df_test, transform=transform)\n",
    "\n",
    "# DataLoader ì„¤ì •\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: torch.Size([32, 3, 224, 224])\n",
      "ë¼ë²¨ ìƒ˜í”Œ: tensor([1, 1, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# í™•ì¸ìš©\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {images.shape}\")  # (ë°°ì¹˜ í¬ê¸°, ì±„ë„, ë†’ì´, ë„ˆë¹„)\n",
    "print(f\"ë¼ë²¨ ìƒ˜í”Œ: {labels[:5]}\")  # ì• 5ê°œ ë¼ë²¨ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (ResNet ì‚¬ìš©)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 5)  # í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ê²Œ ë³€ê²½\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:00<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:22<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:22<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.1292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:21<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:21<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:20<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:21<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:19<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:20<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1242/1242 [31:16<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  # ğŸ”¥ ì´ì œ ì •ìƒ ë™ì‘!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: Loss: 0.4597, Accuracy: 89.35%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Œ ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ê°€ëŠ¥í•˜ë©´ GPU ì‚¬ìš©)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ğŸ“Œ ëª¨ë¸ ë¡œë“œ\n",
    "model = models.resnet18(pretrained=False)  # ğŸ”¹ ì‚¬ì „í•™ìŠµ X (ìƒˆë¡œ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)\n",
    "model.fc = nn.Linear(model.fc.in_features, 5)  # í´ë˜ìŠ¤ ê°œìˆ˜ ë§ì¶”ê¸°\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))  # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model.to(device)\n",
    "model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •\n",
    "\n",
    "# ğŸ“Œ í…ŒìŠ¤íŠ¸ í‰ê°€ ì§„í–‰\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "criterion = nn.CrossEntropyLoss()  # ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "with torch.no_grad():  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° X\n",
    "    for images, labels in DataLoader(testset, batch_size=32, shuffle=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# ğŸ“Œ ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgdataprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
